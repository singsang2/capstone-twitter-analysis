{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Live Streaming Twitter NLP Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project was inspired `sentdex`. (https://github.com/Sentdex/socialsentiment/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is in this repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T16:54:55.951459Z",
     "start_time": "2020-12-08T16:54:55.946576Z"
    }
   },
   "source": [
    "**Jupyter Notebooks**\n",
    "- `final_notebook.ipynb`: contains training and validating of various models including BERT and TF-IDF models.\n",
    "\n",
    "**Python Files**\n",
    "- `twitter_stream.py`: connects to Twitter API and streamline tweets with various keywords\n",
    "- `app.py`: Dash dashboard\n",
    "\n",
    "**Folders**\n",
    "- `images`: contains image files\n",
    "- `models`: contains (1) BERT NLP and (2) spaCy TF-IF vectorization sentiment analysis models *(*BERT model excluded)*\n",
    "- `data`: contains SQLite3 databases pulled from Twitter using Tweepy *(only sample database is included)*\n",
    "- `datasets`: contains datasets that were used to train various NLP sentiment analysis models\n",
    "- `src`: contains useful codes that were used in creating models\n",
    "- `keys`: contains Twitter API key information *(*files excluded)*\n",
    "\n",
    "**How to use this project**\n",
    "\n",
    "1. Add Twitter API information in `keys` directory, and make sure the PATH is correctly defined in `twitter_stream.py`.\n",
    "2. Adjust any keywords or queries in `twitter_stream.py`.\n",
    "3. Set up SQLITE3 database path and run `twitter_stream.py`.\n",
    "4. Run `app.py`!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/dashboard_1.png'>\n",
    "<img src='images/dashboard_2.png'>\n",
    "<img src='images/dashboard_3.png'>\n",
    "<img src='images/dashboard_4.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Live Dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Currently working on deploying the app through Heroku!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Case\n",
    "In the <a href='https://github.com/singsang2/tweet-product-sentiment-analysis'>`previous project`</a>, I got to work with various NLP models analyzing sentiments of tweets on different products. \n",
    "\n",
    "Sources:\n",
    "\n",
    "[1] https://unsplash.com/photos/ulRlAm1ITMU\n",
    "\n",
    "[2]. Bing Liu, https://www.morganclaypool.com/doi/abs/10.2200/s00416ed1v01y201204hlt016\n",
    "\n",
    "## Goals\n",
    "The goals of this project are to \n",
    "\n",
    "    [1] stream Twitter with on various topics (ex. Microsoft, Starbucks, Google, etc.) and\n",
    "    [2] effectively implement various NLP models (including TextBlob, BERT, and TF-IDF models) to classify tweets\n",
    "    [3] to flag the user for any strongly negative tweets on a Dashboard to effectively respond to them.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT model has accuracy of 84%, however due to its computing time,Textblob was used as an initial model to classify polarity of each tweet. The BERT model was used to confirm any strongly negative sentiment tweets classified by Textblob.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Database structure\n",
    "    - As the database size increases, it might get too much for sqlite3 to handle. So, we want to separate database into parts so only when a large number of data is requested, we can combine or join smaller databases. \n",
    "2. Reply feature\n",
    "    - It would be nice if we could add a feature where you could reply to any of tweets shown in the dashboard without going to twitter page.\n",
    "3. Multiple keywords\n",
    "    - It would be nice if multiple keywords can be analyzed and followed at a given time.\n",
    "4. Table Editing Mode\n",
    "    - Modify flagged tweets in the dashboard so that a user can classify flagged tweets as 'resolved', 'false negative', or 'other' for further customer service / data analysis.\n",
    "5. Further analysis on both positive and negative tweets\n",
    "    - Find any correlation between tweet trends with how the company is doing to help with future direction of a company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
